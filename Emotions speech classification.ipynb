{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmoDB - Audio Classification",
      "provenance": [],
      "collapsed_sections": [
        "Qet_3S8o1ODc",
        "5EtC2aDfQ9SV",
        "FQgPMNh0PCDn",
        "DsYvMnqm1eRq",
        "FlDn-dhL04vh",
        "bdxHPpR_SuFO",
        "CHDk4-5bb3Ss",
        "obWeG9txhzDj",
        "wOt_AGlU5EWK",
        "ly9F5HZg5gmJ",
        "-2odLxFxf3R2",
        "Yz9gHdL-0Ru2",
        "LZsneGo-tS1f",
        "CX7xmlVBTME4",
        "2_hLr88V5sjU",
        "RZ86hpApbtbR",
        "JslMMG1Kg1Jo",
        "wQZl3Pe_98RJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#CARICAMENTO LIBRERIE E COLLEGAMENTO DRIVE"
      ],
      "metadata": {
        "id": "Qet_3S8o1ODc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Librerie"
      ],
      "metadata": {
        "id": "5EtC2aDfQ9SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install audiomentations[extras] --quiet\n",
        "!pip install substring --quiet"
      ],
      "metadata": {
        "id": "ayhA2wgrcHl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QtkUnFVBOufU"
      },
      "outputs": [],
      "source": [
        "!PYTHONHASHSEED=0\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "import zipfile\n",
        "from shutil import copyfile\n",
        "from time import time\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as python_random\n",
        "import os\n",
        "np.random.seed(0)\n",
        "python_random.seed(0)\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "import math, random\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from IPython.display import Audio\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchaudio import transforms\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, PolarityInversion, Reverse\n",
        "\n",
        "# Impostazioni di visualizzazione migliorata in Google Colab\n",
        "from IPython.display import Javascript\n",
        "from substring import substringByInd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchvision import transforms as trs\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def resize_colab_cell():\n",
        "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'))\n",
        "get_ipython().events.register('pre_run_cell', resize_colab_cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importazione dati da Google Drive"
      ],
      "metadata": {
        "id": "FQgPMNh0PCDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "ZPw21uyeO_iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copyfile('/content/gdrive/MyDrive/Deep/BerlinSpeech.zip', 'BerlinSpeech.zip')"
      ],
      "metadata": {
        "id": "LdNf3eDYO_f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip = zipfile.ZipFile('BerlinSpeech.zip')\n",
        "zip.extractall()\n",
        "zip.close()"
      ],
      "metadata": {
        "id": "pKK3tDrrO_dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CARICAMENTO DATASET"
      ],
      "metadata": {
        "id": "DsYvMnqm1eRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Il nome del file audio contiene informazioni sul contenuto, come ad esempio l'emozione espressa nell'audio (target) \n",
        "\n",
        "1. I primi due numeri rappresentano l'identificativo dell'attore\n",
        "2. La stringa alfanumerica di 3 caratteri in mezzo identifica la frase\n",
        "3. Le ultime due le iniziali del sentimento, la prima riferita all'inglese e la seconda al tedesco. \n"
      ],
      "metadata": {
        "id": "zASZP5kYLYKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Si inizializzano i dizionari con la quale si ricaverÃ  poi il target per ogni audio, a partire dal nome del file.\n",
        "de_emotions = { 'W': 'anger',\n",
        "                'L': 'boredom',\n",
        "                'E': 'disgust',\n",
        "                'A': 'fear',\n",
        "                'F': 'happiness',\n",
        "                'T': 'sadness',\n",
        "                'N': 'neutral',\n",
        "                }\n",
        "\n",
        "mapping = {'anger':0,\n",
        "           'boredom':1,\n",
        "           'disgust':2,\n",
        "           'fear':3,\n",
        "           'happiness':4,\n",
        "           'sadness':5,\n",
        "           'neutral':6,\n",
        "           }"
      ],
      "metadata": {
        "id": "SqH8xEsSO_as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "audio = []\n",
        "label = []\n",
        "actor_label = []\n",
        "for idx,speech_name in enumerate(os.listdir('/content/wav')):\n",
        "  audio.append('/wav/' + speech_name)\n",
        "  genre = de_emotions[speech_name[5]]\n",
        "  actor = substringByInd(str(speech_name), 0, 1)\n",
        "  label.append(mapping[genre])\n",
        "  actor_label.append(actor)\n",
        "\n",
        "\n",
        "df['audio'] = audio\n",
        "df['etichetta'] = label\n",
        "df['attore'] = actor_label\n",
        "df['etichett-attore'] = df['etichetta'].astype(str) + df['attore'].astype(str)\n",
        "unique_authors = df['etichett-attore'].value_counts()==1\n",
        "unique_authors = unique_authors[unique_authors].index\n",
        "for index in unique_authors:\n",
        "  df['etichett-attore'][df['etichett-attore']==index]='999'\n",
        "df"
      ],
      "metadata": {
        "id": "RBy6aHkNjL1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FUNZIONI PER IL PREPROCESSING E PER L'ALLENAMENTO DELLA RETE"
      ],
      "metadata": {
        "id": "FlDn-dhL04vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility per la gestione delle tracce audio"
      ],
      "metadata": {
        "id": "bdxHPpR_SuFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioUtil():\n",
        "\n",
        "  # ----------------------------\n",
        "  # Load an audio file. Return the signal as a tensor and the sample rate\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def open(audio_file):\n",
        "    sig, sr = torchaudio.load(audio_file)\n",
        "    return (sig, sr)\n",
        "\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert the given audio to the desired number of channels\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def rechannel(aud, new_channel):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sig.shape[0] == new_channel):\n",
        "        # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    if (new_channel == 1):\n",
        "        # Convert from stereo to mono by selecting only the first channel\n",
        "      resig = sig[:1, :]\n",
        "    else:\n",
        "        # Convert from mono to stereo by duplicating the first channel\n",
        "      resig = torch.cat([sig, sig])\n",
        "\n",
        "    return ((resig, sr))\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Since Resample applies to a single channel, we resample one channel at a time\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def resample(aud, newsr):\n",
        "    sig, sr = aud\n",
        "\n",
        "    if (sr == newsr):\n",
        "        # Nothing to do\n",
        "      return aud\n",
        "\n",
        "    num_channels = sig.shape[0]\n",
        "      # Resample first channel\n",
        "    resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
        "    if (num_channels > 1):\n",
        "        # Resample the second channel and merge both channels\n",
        "      retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
        "      resig = torch.cat([resig, retwo])\n",
        "\n",
        "    return ((resig, newsr))\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def pad_trunc(aud, max_ms):\n",
        "    sig, sr = aud\n",
        "    num_rows, sig_len = sig.shape\n",
        "    max_len = sr//1000 * max_ms\n",
        "\n",
        "    if (sig_len > max_len):\n",
        "        # Truncate the signal to the given length\n",
        "      sig = sig[:,:max_len]\n",
        "\n",
        "    elif (sig_len < max_len):\n",
        "        # Length of padding to add at the beginning and end of the signal\n",
        "      pad_begin_len = random.randint(0, max_len - sig_len)\n",
        "      pad_end_len = max_len - sig_len - pad_begin_len\n",
        "\n",
        "        # Pad with 0s\n",
        "      pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
        "      pad_end = torch.zeros((num_rows, pad_end_len))\n",
        "\n",
        "      sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
        "        \n",
        "    return (sig, sr)\n",
        "\n",
        "\n",
        "\n",
        "   # ----------------------------\n",
        "  # Shifts the signal to the left or right by some percent. Values at the end\n",
        "  # are 'wrapped around' to the start of the transformed signal.\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def time_shift(aud, shift_limit):\n",
        "    sig,sr = aud\n",
        "    _, sig_len = sig.shape\n",
        "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
        "    return (sig.roll(shift_amt), sr)\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Generate a Spectrogram\n",
        "  # ----------------------------\n",
        "  @staticmethod  \n",
        "  def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
        "    sig,sr = aud\n",
        "    top_db = 80\n",
        "\n",
        "      # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
        "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
        "\n",
        "      # Convert to decibels\n",
        "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
        "    return spec"
      ],
      "metadata": {
        "id": "ORTJDsteO_Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione che implementa l'augmentation "
      ],
      "metadata": {
        "id": "CHDk4-5bb3Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augmented_train(df):\n",
        "\n",
        "  lista = [AddGaussianNoise(p=1, min_amplitude=0.01, max_amplitude=0.02),\n",
        "            TimeStretch(p=1, min_rate=0.8, max_rate=0.9),\n",
        "            TimeStretch(p=1, min_rate=1.125, max_rate=1.25),\n",
        "            PitchShift(p=1, min_semitones=1, max_semitones=3),\n",
        "            PitchShift(p=1, min_semitones=-3, max_semitones=-1),\n",
        "            Shift(p=1, fade=True),  #fade_duration=0.01\n",
        "            PolarityInversion(p=1),\n",
        "            Reverse(p=1)]\n",
        "\n",
        "  df_block = pd.DataFrame([], columns=['audio', 'etichetta'])\n",
        "\n",
        "  for idx,row in enumerate(df.iterrows()):\n",
        "    audio_file = '/content' + row[1][0]\n",
        "    aud = AudioUtil.open(audio_file)\n",
        "    class_id = row[1][1]\n",
        "    \n",
        "\n",
        "    reaud = AudioUtil.resample(aud, 22050)\n",
        "    rechan = AudioUtil.rechannel(reaud, 1)\n",
        "    dur_aud = AudioUtil.pad_trunc(rechan, 4000)\n",
        "    torchaudio.save(audio_file,dur_aud[0], 22050)\n",
        "\n",
        "    functions = list(np.random.choice(a = lista, replace = False, size = 3, p=[1/6,1/12,1/12,1/12,1/12,1/6,1/6,1/6]))\n",
        "   \n",
        "    for index, func in enumerate(functions):\n",
        "      augment = Compose([func])\n",
        "      if (func != lista[1]) & (func != lista[2]):     \n",
        "        augmented_samples = augment(samples = dur_aud[0], sample_rate=22050)\n",
        "      else:\n",
        "        dur_aud_1 = np.asarray(dur_aud[0])                   # Time Stretch vuole in input un ndarray e non un tensore...\n",
        "        augmented_samples = augment(samples = dur_aud_1, sample_rate=22050)\n",
        "        augmented_samples = torch.from_numpy(augmented_samples)\n",
        "\n",
        "      if torch.is_tensor(augmented_samples)== False:\n",
        "        augmented_samples = torch.from_numpy(augmented_samples.copy())\n",
        "      augmented_samples\n",
        "      augmented_filename = '/content'  + substringByInd(str(row[1][0]), 0, 11) + '_aug' + str(index) + '.wav' \n",
        "      torchaudio.save(augmented_filename,augmented_samples,22050)"
      ],
      "metadata": {
        "id": "0JIZxhlTb3k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train(df)"
      ],
      "metadata": {
        "id": "dKmNfAgAgpvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzioni che implementano gli spettrogrammi"
      ],
      "metadata": {
        "id": "obWeG9txhzDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_spectrogram(df):\n",
        "\n",
        "  df_block = pd.DataFrame([], columns=['audio', 'etichetta'])\n",
        "\n",
        "  for idx,row in enumerate(df.iterrows()):\n",
        "    audio_file = '/content' + row[1][0]\n",
        "    aud = AudioUtil.open(audio_file)\n",
        "    class_id = row[1][1]    \n",
        "    spectrogram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    data = {'audio': spectrogram, 'etichetta': class_id}\n",
        "    df_block = df_block.append(data, ignore_index = True)\n",
        "    for index in range(3):\n",
        "      audio_file =  '/content'  + substringByInd(str(row[1][0]), 0, 11) + '_aug' + str(index) + '.wav'\n",
        "      aud = AudioUtil.open(audio_file)\n",
        "      spectrogram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "      data = {'audio': spectrogram, 'etichetta': class_id}\n",
        "      df_block = df_block.append(data, ignore_index = True)\n",
        "  return df_block"
      ],
      "metadata": {
        "id": "Pou28qZOhzSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_spectrogram_validation(df):\n",
        "\n",
        "  df_block = pd.DataFrame([], columns=['audio', 'etichetta'])\n",
        "\n",
        "  for idx,row in enumerate(df.iterrows()):\n",
        "    audio_file = '/content' + row[1][0]\n",
        "    aud = AudioUtil.open(audio_file)\n",
        "    class_id = row[1][1]    \n",
        "    spectrogram = AudioUtil.spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None)\n",
        "    data = {'audio': spectrogram, 'etichetta': class_id}\n",
        "    df_block = df_block.append(data, ignore_index = True)\n",
        "    \n",
        "  return df_block"
      ],
      "metadata": {
        "id": "aihtUAJIpcmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzione che implementa l'inferenza del modello sul validation"
      ],
      "metadata": {
        "id": "wOt_AGlU5EWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Inference\n",
        "# ----------------------------\n",
        "\n",
        "def inference (dataset, model, criterion, val_dl, batch__size):\n",
        "  correct_prediction = 0\n",
        "  total_prediction = 0\n",
        "  running_loss_val = 0\n",
        "  acc_diz = {0:[0,0,0],1:[0,0,0],2:[0,0,0],3:[0,0,0],4:[0,0,0],5:[0,0,0],6:[0,0,0]}\n",
        " \n",
        "  # Disable gradient updates\n",
        "  with torch.no_grad():\n",
        "    for data in val_dl:\n",
        "      # Get the input features and target labels, and put them on the GPU\n",
        "\n",
        "      test_spec = compute_spectrogram_validation(dataset.iloc[data.tolist()])\n",
        "      test_sound = SoundDS(test_spec)\n",
        "      test_tensor = torch.utils.data.DataLoader(test_sound, batch_size=batch__size)\n",
        "      for i in test_tensor:\n",
        "        tt = i\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "      inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "      # Normalize the inputs\n",
        "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "      inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "\n",
        "      # Get predictions\n",
        "      \n",
        "      outputs = model(inputs)\n",
        "      val_loss = criterion(outputs, labels).item()\n",
        "      running_loss_val += val_loss\n",
        "\n",
        "      # Get the predicted class with the highest score\n",
        "      _, prediction = torch.max(outputs,1)\n",
        "      \n",
        "      # Count of predictions that matched the target label\n",
        "      correct_prediction += (prediction == labels).sum().item()\n",
        "      total_prediction += prediction.shape[0]\n",
        "\n",
        "      for label_index,label in enumerate(labels):\n",
        "          acc_diz[int(label)][0] += 1 \n",
        "          acc_diz[int(label)][1] += int(label == prediction[label_index])\n",
        "          acc_diz[int(prediction[label_index])][2] += 1      \n",
        "  num_batches_val = len(val_dl)\n",
        "  avg_loss_val = running_loss_val / num_batches_val\n",
        "  acc = correct_prediction/total_prediction\n",
        "  print(f'Validation Accuracy: {acc:.2f}, Validation Loss: {avg_loss_val:.2f},Total items: {total_prediction}')\n",
        "  return acc, avg_loss_val, acc_diz"
      ],
      "metadata": {
        "id": "-BpOg3Ki5CoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funzioni che valutano l'andamento dell'accuracy e della loss di train e validation durante le epoche di addestramento"
      ],
      "metadata": {
        "id": "ly9F5HZg5gmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_acc(training, validation): \n",
        "\n",
        "# Sintesi risultati K-fold \n",
        "    if isinstance(training[0], list):\n",
        "      # Taglio al numero di epoche minimo nelle 3 fold (Early Stopping)\n",
        "      training = [training[j][0:(min([len(training[i]) for i in range(len(training))]))] for j in range(len(training))]\n",
        "      validation = [validation[j][0:(min([len(validation[i]) for i in range(len(validation))]))] for j in range(len(validation))]\n",
        "      acc = np.mean(np.array(training), axis = 0)\n",
        "      val_acc = np.mean(np.array(validation), axis = 0)\n",
        "# Modello iniziale senza K-fold\n",
        "    else:\n",
        "      acc = training\n",
        "      val_acc = validation\n",
        "    epochs = range(len(acc))\n",
        "    figure = plt.figure()\n",
        "    plt.style.use('dark_background')\n",
        "    plt.plot(epochs, acc, \"b\", label=\"Accuratezza Training\")\n",
        "    plt.plot(epochs, val_acc, \"r\", label=\"Accuratezza Validation\")\n",
        "    plt.title(\"Accuratezza Training e Validation\")\n",
        "    plt.xlabel(\"Epoca\",color='white')\n",
        "    plt.ylabel(\"Accuratezza\",color='white')\n",
        "    plt.xticks(range(0,len(acc),3))\n",
        "    figure.set_facecolor('black')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def visualize_loss(training, validation):\n",
        "\n",
        "# Sintesi risultati K-fold  \n",
        "    if isinstance(training[0], list):\n",
        "      # Taglio al numero di epoche minimo nelle 3 fold (Early Stopping)\n",
        "      training = [training[j][0:(min([len(training[i]) for i in range(len(training))]))] for j in range(len(training))]\n",
        "      validation = [validation[j][0:(min([len(validation[i]) for i in range(len(validation))]))] for j in range(len(validation))]\n",
        "      loss = np.mean(np.array(training), axis = 0)\n",
        "      val_loss = np.mean(np.array(validation), axis = 0)\n",
        "# Modello iniziale senza K-fold\n",
        "    else:\n",
        "      loss = training\n",
        "      val_loss = validation\n",
        "    epochs = range(len(loss))\n",
        "    figure = plt.figure()\n",
        "    figure.set_facecolor('black')\n",
        "    plt.style.use('dark_background')\n",
        "    plt.plot(epochs, loss, \"b\", label=\"Loss Training\")\n",
        "    plt.plot(epochs, val_loss, \"r\", label=\"Loss Validation\")\n",
        "    plt.title(\"Loss Training e Validation\")\n",
        "    plt.xlabel(\"Epoca\",color='white')\n",
        "    plt.ylabel(\"Loss\",color='white')\n",
        "    plt.xticks(range(0,len(loss),3))\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "H1bVAn_B5hSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "def show_values(axs, orient=\"v\", space=.01):\n",
        "    def _single(ax):\n",
        "        if orient == \"v\":\n",
        "            for p in ax.patches:\n",
        "                _x = p.get_x() + p.get_width() / 2\n",
        "                _y = p.get_y() + p.get_height() + (p.get_height()*0.01)\n",
        "                value = '{:.3f}'.format(p.get_height())\n",
        "                ax.text(_x, _y, value, ha=\"center\") \n",
        "        elif orient == \"h\":\n",
        "            for p in ax.patches:\n",
        "                _x = p.get_x() + p.get_width() + float(space)\n",
        "                _y = p.get_y() + p.get_height() - (p.get_height()*0.50 - 0.1)\n",
        "                value = '{:.3f}'.format(p.get_width())\n",
        "                ax.text(_x, _y, value, ha=\"left\")\n",
        "\n",
        "    if isinstance(axs, np.ndarray):\n",
        "        for idx, ax in np.ndenumerate(axs):\n",
        "            _single(ax)\n",
        "    else:\n",
        "        _single(axs)\n",
        "\n",
        "def plot_class_recall(acc_diz):\n",
        "  acc_list = []\n",
        "  sentiment_list = ['rabbia','noia','disgusto','paura','felicitÃ ','tristezza','neutrale']\n",
        "  for index_, vals in acc_diz.items():\n",
        "    acc_list.append(vals[1]/vals[0])\n",
        "  \n",
        "  sns.set(font_scale = 1.2, rc={'figure.figsize':(7,5),'axes.facecolor':'black', 'figure.facecolor':'black','axes.linewidth': 1,'ytick.color': 'white','xtick.color': 'white','axes.edgecolor': 'grey','axes.grid': False, 'text.color': 'white', 'axes.spines.right': False, 'axes.spines.top': False, 'patch.linewidth': 0.7})\n",
        "  bar=sns.barplot(x=acc_list, y=sentiment_list, palette='husl')\n",
        "  show_values(bar,\"h\", space=0.01)"
      ],
      "metadata": {
        "id": "oE55fnq5QV92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_class_precision_and_recall(acc_diz,acc_finale):\n",
        "  rec_prec_df = pd.DataFrame(columns = ['Sentimento', 'Misura', 'Valore'])\n",
        "  sentiment_list =['rabbia','noia','disgusto','paura','felicitÃ ','tristezza','neutrale']\n",
        "  for index_, vals in acc_diz.items():\n",
        "    if vals[2]>0:\n",
        "      rec = {'Sentimento':sentiment_list[index_], 'Misura':'Recall', 'Valore':(vals[1]/vals[0])}\n",
        "      prec = {'Sentimento':sentiment_list[index_], 'Misura':'Precision', 'Valore':(vals[1]/vals[2])}\n",
        "      rec_prec_df = rec_prec_df.append(rec, ignore_index=True)\n",
        "      rec_prec_df = rec_prec_df.append(prec, ignore_index=True)\n",
        "    else:\n",
        "      rec = {'Sentimento':sentiment_list[index_], 'Misura':'Recall', 'Valore':vals[1]/vals[0]}\n",
        "      prec = {'Sentimento':sentiment_list[index_], 'Misura':'Precision', 'Valore':np.inf}\n",
        "      rec_prec_df = rec_prec_df.append(rec, ignore_index=True)\n",
        "      rec_prec_df = rec_prec_df.append(prec, ignore_index=True)\n",
        "      \n",
        "  sns.set(font_scale = 1.2, rc={'figure.figsize':(11.5,8),'axes.facecolor':'black','axes.labelcolor':'white', 'figure.facecolor':'black','axes.linewidth': 1,'ytick.color': 'white','xtick.color': 'white','axes.edgecolor': 'grey','axes.grid': False, 'text.color': 'white', 'axes.spines.right': False, 'axes.spines.top': False, 'patch.linewidth': 0.5})\n",
        "  bar=sns.barplot(x='Valore', y='Sentimento', hue='Misura', data=rec_prec_df, palette='mako')\n",
        "  plt.legend(bbox_to_anchor=(0.95, 0.7), loc='upper left', borderaxespad=0)\n",
        "  plt.axvline(x=acc_finale,ymin=0,ymax=1,color='pink')\n",
        "  show_values(bar,\"h\", space=0.02)"
      ],
      "metadata": {
        "id": "xMCLe004Ppkt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "919ac900-c9e4-42e9-d205-0d9b04fd959f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Si inizializza una classe speciale che servirÃ  da input al Data Loader"
      ],
      "metadata": {
        "id": "-2odLxFxf3R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoundDS(Dataset):\n",
        "  def __init__(self, df):\n",
        "    self.df = df         \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)    \n",
        "     \n",
        " \n",
        "  def __getitem__(self, idx):\n",
        "    \n",
        "    audio_file = self.df.loc[idx, 'audio']\n",
        "    \n",
        "    class_id = self.df.loc[idx, 'etichetta']\n",
        "\n",
        "    return audio_file, class_id"
      ],
      "metadata": {
        "id": "M200fIccO_QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COSTRUZIONE RETI E TRAINING MODELLI"
      ],
      "metadata": {
        "id": "Yz9gHdL-0Ru2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello 1: CNN a 2 layer - Batch 16"
      ],
      "metadata": {
        "id": "LZsneGo-tS1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier0 (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        init.xavier_normal_(self.conv1.weight)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1, self.pool1]\n",
        "\n",
        "        # Second Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        init.xavier_normal_(self.conv2.weight)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2, self.pool2]\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=32, out_features=7)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.shape[0], -1)  #flattening\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "        x = self.soft(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel0 = AudioClassifier0()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel0 = myModel0.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel0.parameters()).device"
      ],
      "metadata": {
        "id": "zOz7rYS_5t8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training0(model, dataset, num_epochs, batch__size):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
        "                                                steps_per_epoch=int(len(dataset)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "  for layer in model.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "\n",
        "  # Random split of 2/3:1/3 between training and validation\n",
        "  num_items = len(dataset)\n",
        "  num_train = round(num_items * 2/3)\n",
        "  num_val = num_items - num_train\n",
        "  train_ds, val_ds, y_train, y_test = train_test_split(dataset.index, dataset['etichett-attore'],test_size=num_val, train_size=num_train,shuffle = True, random_state=123, stratify=dataset['etichett-attore'])\n",
        "\n",
        "\n",
        "  # Create training and validation data loaders\n",
        "  train_dl = torch.utils.data.DataLoader(train_ds, batch_size=int(batch__size/4), shuffle = True)\n",
        "  val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch__size, shuffle = True)\n",
        "  accuracy_train = []\n",
        "  accuracy_val = []\n",
        "  loss_train = []\n",
        "  loss_validation = []\n",
        "  acc_diz_list = []\n",
        "\n",
        "  # Repeat for each epoch\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_vloss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Repeat for each batch in the training set\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        train_spec = compute_spectrogram(dataset.iloc[data.tolist()])\n",
        "        train_sound = SoundDS(train_spec)\n",
        "        train_tensor = torch.utils.data.DataLoader(train_sound, batch_size=batch__size)\n",
        "        for i in train_tensor:\n",
        "          tt = i\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "        inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Keep stats for Loss and Accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Get the predicted class with the highest score (posterior, predicted labels)\n",
        "        _, prediction = torch.max(outputs,1)\n",
        "    \n",
        "        # Count of predictions that matched the target label\n",
        "        correct_prediction += (prediction == labels).sum().item()\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "      # Print stats at the end of the epoch\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    \n",
        "    acc = correct_prediction/total_prediction\n",
        "    acc_val, loss_val, acc_diz = inference(dataset, model, criterion, val_dl, batch__size)\n",
        "    \n",
        "    accuracy_train.append(acc)\n",
        "    accuracy_val.append(acc_val)\n",
        "    loss_train.append(avg_loss)\n",
        "    loss_validation.append(loss_val)\n",
        "    acc_diz_list.append(acc_diz)\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', f'{avg_loss:.2f}', 'Accuracy:', acc)\n",
        "\n",
        "  return accuracy_train, accuracy_val, loss_train, loss_validation, acc_diz_list"
      ],
      "metadata": {
        "id": "5SocI7D053LN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 245\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "num_epochs= 50\n",
        "batch__size = 16\n",
        "accuracy0, accuracy_val0, loss0, loss_val0, acc_diz0 = training0(myModel0, df, num_epochs, batch__size)"
      ],
      "metadata": {
        "id": "aTWfUHmz58WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_acc(accuracy0,accuracy_val0)"
      ],
      "metadata": {
        "id": "LrTi_LtsTzYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_loss(loss0,loss_val0)"
      ],
      "metadata": {
        "id": "00Px68fa4i15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_min = np.argmin(loss_val0)\n",
        "index_min\n",
        "plot_class_precision_and_recall(acc_diz0[index_min],accuracy_val0[index_min])"
      ],
      "metadata": {
        "id": "nwTqx7zTRD8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello 2: CNN a 3 layer - Batch 16"
      ],
      "metadata": {
        "id": "CX7xmlVBTME4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier2 (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        init.xavier_normal_(self.conv1.weight)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1, self.pool1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        init.xavier_normal_(self.conv2.weight)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2, self.pool2]\n",
        "\n",
        "        # Third Convolution Block\n",
        "        self.conv3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        init.xavier_normal_(self.conv3.weight)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=128, out_features=7)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        " \n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "        x = self.soft(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel2 = AudioClassifier2()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel2 = myModel2.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel2.parameters()).device"
      ],
      "metadata": {
        "id": "S9JI55SpbuvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training2(model, dataset, num_epochs, batch__size):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=0.01, weight_decay=0.001)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
        "                                                steps_per_epoch=int(len(dataset)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "  for layer in model.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "\n",
        "  # Random split of 2/3:1/3 between training and validation\n",
        "  num_items = len(dataset)\n",
        "  num_train = round(num_items * 2/3)\n",
        "  num_val = num_items - num_train\n",
        "  train_ds, val_ds, y_train, y_test = train_test_split(dataset.index, dataset['etichett-attore'],test_size=num_val, train_size=num_train,shuffle = True, random_state=123, stratify=dataset['etichett-attore'])\n",
        "\n",
        "\n",
        "  # Create training and validation data loaders\n",
        "  train_dl = torch.utils.data.DataLoader(train_ds, batch_size=int(batch__size/4), shuffle = True)\n",
        "  val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch__size, shuffle = True)\n",
        "  accuracy_train = []\n",
        "  accuracy_val = []\n",
        "  loss_train = []\n",
        "  loss_validation = []\n",
        "  acc_diz_list = []\n",
        "\n",
        "  # Repeat for each epoch\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_vloss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Repeat for each batch in the training set\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        train_spec = compute_spectrogram(dataset.iloc[data.tolist()])\n",
        "        train_sound = SoundDS(train_spec)\n",
        "        train_tensor = torch.utils.data.DataLoader(train_sound, batch_size=batch__size)\n",
        "        for i in train_tensor:\n",
        "          tt = i\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "        inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Keep stats for Loss and Accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Get the predicted class with the highest score (posterior, predicted labels)\n",
        "        _, prediction = torch.max(outputs,1)\n",
        "    \n",
        "        # Count of predictions that matched the target label\n",
        "        correct_prediction += (prediction == labels).sum().item()\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "      # Print stats at the end of the epoch\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    \n",
        "    acc = correct_prediction/total_prediction\n",
        "    acc_val, loss_val, acc_diz = inference(dataset, model, criterion, val_dl, batch__size)\n",
        "    \n",
        "    accuracy_train.append(acc)\n",
        "    accuracy_val.append(acc_val)\n",
        "    loss_train.append(avg_loss)\n",
        "    loss_validation.append(loss_val)\n",
        "    acc_diz_list.append(acc_diz)\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', f'{avg_loss:.2f}', 'Accuracy:', acc)\n",
        "\n",
        "  return accuracy_train, accuracy_val, loss_train, loss_validation, acc_diz_list"
      ],
      "metadata": {
        "id": "zH9-M4W-4ora",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "bb7bf79d-9af1-4ef0-f29d-8a6bb488303f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 784\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch__size = 16\n",
        "num_epochs= 50\n",
        "accuracy2, accuracy_val2, loss2, loss_val2, acc_diz2 = training2(myModel2, df, num_epochs, batch__size)"
      ],
      "metadata": {
        "id": "fS6qiLVA5a16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_acc(accuracy2, accuracy_val2)"
      ],
      "metadata": {
        "id": "Drgmay2eWt5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_loss(loss2, loss_val2)"
      ],
      "metadata": {
        "id": "vY1pRfXQOpr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_min = np.argmin(loss_val2)\n",
        "index_min\n",
        "plot_class_precision_and_recall(acc_diz2[index_min],accuracy_val2[index_min])"
      ],
      "metadata": {
        "id": "VnNrDf5ASSq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello 3: CNN a 3 layer - Batch 32"
      ],
      "metadata": {
        "id": "2_hLr88V5sjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier3 (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        init.xavier_normal_(self.conv1.weight)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1, self.pool1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        init.xavier_normal_(self.conv2.weight)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2, self.pool2]\n",
        "\n",
        "        # Third Convolution Block\n",
        "        self.conv3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2,2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        init.xavier_normal_(self.conv3.weight)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=128, out_features=7)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        " \n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "        x = self.soft(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel3 = AudioClassifier3()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel3 = myModel3.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel3.parameters()).device"
      ],
      "metadata": {
        "id": "XAekClI45-Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training3(model, dataset, num_epochs, batch__size):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=0.01, weight_decay=0.005)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
        "                                                steps_per_epoch=int(len(dataset)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "  for layer in model.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "\n",
        "  # Random split of 2/3:1/3 between training and validation\n",
        "  num_items = len(dataset)\n",
        "  num_train = round(num_items * 2/3)\n",
        "  num_val = num_items - num_train\n",
        "  train_ds, val_ds, y_train, y_test = train_test_split(dataset.index, dataset['etichett-attore'],test_size=num_val, train_size=num_train,shuffle = True, random_state=123, stratify=dataset['etichett-attore'])\n",
        "\n",
        "\n",
        "  # Create training and validation data loaders\n",
        "  train_dl = torch.utils.data.DataLoader(train_ds, batch_size=int(batch__size/4), shuffle = True)\n",
        "  val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch__size, shuffle = True)\n",
        "  accuracy_train = []\n",
        "  accuracy_val = []\n",
        "  loss_train = []\n",
        "  loss_validation = []\n",
        "  acc_diz_list = []\n",
        "\n",
        "  # early_stopping = \n",
        "  # Repeat for each epoch\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_vloss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Repeat for each batch in the training set\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        train_spec = compute_spectrogram(dataset.iloc[data.tolist()])\n",
        "        train_sound = SoundDS(train_spec)\n",
        "        train_tensor = torch.utils.data.DataLoader(train_sound, batch_size=batch__size)\n",
        "        for i in train_tensor:\n",
        "          tt = i\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "        inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Keep stats for Loss and Accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Get the predicted class with the highest score (posterior, predicted labels)\n",
        "        _, prediction = torch.max(outputs,1)\n",
        "    \n",
        "        # Count of predictions that matched the target label\n",
        "        correct_prediction += (prediction == labels).sum().item()\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "      # Print stats at the end of the epoch\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    \n",
        "    acc = correct_prediction/total_prediction\n",
        "    acc_val, loss_val, acc_diz = inference(dataset, model, criterion, val_dl, batch__size)\n",
        "    acc_diz_list.append(acc_diz)\n",
        "    accuracy_train.append(acc)\n",
        "    accuracy_val.append(acc_val)\n",
        "    loss_train.append(avg_loss)\n",
        "    loss_validation.append(loss_val)\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', f'{avg_loss:.2f}', 'Accuracy:', acc)\n",
        "\n",
        "  return accuracy_train, accuracy_val, loss_train, loss_validation, acc_diz_list"
      ],
      "metadata": {
        "id": "mVvvpTGx6UEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 245\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch__size = 32\n",
        "num_epochs= 50\n",
        "accuracy3, accuracy_val3, loss3, loss_val3, acc_diz3 = training3(myModel3, df, num_epochs, batch__size)"
      ],
      "metadata": {
        "id": "w1M284RP6kd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_acc(accuracy3, accuracy_val3)"
      ],
      "metadata": {
        "id": "OMuEEQB7Zc_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_loss(loss3,loss_val3)"
      ],
      "metadata": {
        "id": "4AyO0WPpOP4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_min = np.argmin(loss_val3)\n",
        "index_min\n",
        "plot_class_precision_and_recall(acc_diz3[index_min],accuracy_val3[index_min])"
      ],
      "metadata": {
        "id": "xhlrQCV4ScEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modello 4: CNN a 4 layer - Batch 32"
      ],
      "metadata": {
        "id": "RZ86hpApbtbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier4 (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        init.xavier_normal_(self.conv1.weight)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1, self.pool1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        init.xavier_normal_(self.conv2.weight)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2, self.pool2]\n",
        "\n",
        "        # Third Convolution Block\n",
        "        self.conv3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.pool3 = nn.MaxPool2d(2,2)\n",
        "        init.xavier_normal_(self.conv3.weight)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3, self.pool3]\n",
        "\n",
        "        # Fourth Convolution Block\n",
        "        self.conv4 = nn.Conv2d(128, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        init.xavier_normal_(self.conv4.weight)\n",
        "        self.conv4.bias.data.zero_()\n",
        "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=512, out_features=7)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        " \n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "        x = self.soft(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel4 = AudioClassifier4()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel4 = myModel4.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel4.parameters()).device"
      ],
      "metadata": {
        "id": "4dEU6eW18P1u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b29673d-7fe8-4521-cc7c-5eaf321d702e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training4(model, dataset, num_epochs, batch__size):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=0.002, weight_decay=0.01)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002,\n",
        "                                                steps_per_epoch=int(len(dataset)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "  for layer in model.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "\n",
        "  # Random split of 2/3:1/3 between training and validation\n",
        "  num_items = len(dataset)\n",
        "  num_train = round(num_items * 2/3)\n",
        "  num_val = num_items - num_train\n",
        "  train_ds, val_ds, y_train, y_test = train_test_split(dataset.index, dataset['etichett-attore'],test_size=num_val, train_size=num_train,shuffle = True, random_state=123, stratify=dataset['etichett-attore'])\n",
        "\n",
        "\n",
        "  # Create training and validation data loaders\n",
        "  train_dl = torch.utils.data.DataLoader(train_ds, batch_size=int(batch__size/4), shuffle = True)\n",
        "  val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch__size, shuffle = True)\n",
        "  accuracy_train = []\n",
        "  accuracy_val = []\n",
        "  loss_train = []\n",
        "  loss_validation = []\n",
        "  acc_diz_list = []\n",
        "\n",
        "  # Repeat for each epoch\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_vloss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Repeat for each batch in the training set\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        train_spec = compute_spectrogram(dataset.iloc[data.tolist()])\n",
        "        train_sound = SoundDS(train_spec)\n",
        "        train_tensor = torch.utils.data.DataLoader(train_sound, batch_size=batch__size)\n",
        "        for i in train_tensor:\n",
        "          tt = i\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "        inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Keep stats for Loss and Accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Get the predicted class with the highest score (posterior, predicted labels)\n",
        "        _, prediction = torch.max(outputs,1)\n",
        "    \n",
        "        # Count of predictions that matched the target label\n",
        "        correct_prediction += (prediction == labels).sum().item()\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "      # Print stats at the end of the epoch\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    \n",
        "    acc = correct_prediction/total_prediction\n",
        "    acc_val, loss_val, acc_diz = inference(dataset, model, criterion, val_dl, batch__size)\n",
        "    \n",
        "    accuracy_train.append(acc)\n",
        "    accuracy_val.append(acc_val)\n",
        "    loss_train.append(avg_loss)\n",
        "    loss_validation.append(loss_val)\n",
        "    acc_diz_list.append(acc_diz)\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', f'{avg_loss:.2f}', 'Accuracy:', acc)\n",
        "\n",
        "  return accuracy_train, accuracy_val, loss_train, loss_validation, acc_diz_list"
      ],
      "metadata": {
        "id": "ACFRRHrCGtfy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1cb12e40-8168-40e9-c61a-2dcf27b2a5a4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 245\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch__size = 32\n",
        "num_epochs= 50\n",
        "accuracy4, accuracy_val4, loss4, loss_val4, acc_diz4 = training4(myModel4, df, num_epochs, batch__size)"
      ],
      "metadata": {
        "id": "YM8dUvaT8Hfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_acc(accuracy4, accuracy_val4)"
      ],
      "metadata": {
        "id": "-SWVCWMOa0s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_loss(loss4, loss_val4)"
      ],
      "metadata": {
        "id": "JWiz0RJJSzzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_min = np.argmin(loss_val4)\n",
        "index_min\n",
        "plot_class_precision_and_recall(acc_diz4[index_min],accuracy_val4[index_min])"
      ],
      "metadata": {
        "id": "JapMERkzSskv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modello finale: CNN a 3 layer - Batch 32 + Early stopping\n",
        "\n"
      ],
      "metadata": {
        "id": "JslMMG1Kg1Jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier5 (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        init.xavier_normal_(self.conv1.weight)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1, self.pool1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        init.xavier_normal_(self.conv2.weight)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2, self.pool2]\n",
        "\n",
        "        # Third Convolution Block\n",
        "        self.conv3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        init.xavier_normal_(self.conv3.weight)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=128, out_features=7)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        " \n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "        x = self.soft(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel5 = AudioClassifier5()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel5 = myModel5.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel5.parameters()).device"
      ],
      "metadata": {
        "id": "Kkbd7GmDg2Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training5(model, dataset, num_epochs, batch__size):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=0.01, weight_decay=0.005)\n",
        "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
        "                                                steps_per_epoch=int(len(dataset)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "  for layer in model.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "\n",
        "  # Random split of 2/3:1/3 between training and validation\n",
        "  num_items = len(dataset)\n",
        "  num_train = round(num_items * 2/3)\n",
        "  num_val = num_items - num_train\n",
        "  train_ds, val_ds, y_train, y_test = train_test_split(dataset.index, dataset['etichett-attore'],test_size=num_val, train_size=num_train,shuffle = True, random_state=123, stratify=dataset['etichett-attore'])\n",
        "\n",
        "\n",
        "  # Create training and validation data loaders\n",
        "  train_dl = torch.utils.data.DataLoader(train_ds, batch_size=int(batch__size/4), shuffle = True)\n",
        "  val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batch__size, shuffle = True)\n",
        "  accuracy_train = []\n",
        "  accuracy_val = []\n",
        "  loss_train = []\n",
        "  loss_validation = []\n",
        "\n",
        "  # Repeat for each epoch\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    running_vloss = 0.0\n",
        "    correct_prediction = 0\n",
        "    total_prediction = 0\n",
        "\n",
        "    # Repeat for each batch in the training set\n",
        "    for i, data in enumerate(train_dl):\n",
        "\n",
        "        train_spec = compute_spectrogram(dataset.iloc[data.tolist()])\n",
        "        train_sound = SoundDS(train_spec)\n",
        "        train_tensor = torch.utils.data.DataLoader(train_sound, batch_size=batch__size)\n",
        "        for i in train_tensor:\n",
        "          tt = i\n",
        "        # Get the input features and target labels, and put them on the GPU\n",
        "        inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "        # Normalize the inputs\n",
        "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "        inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Keep stats for Loss and Accuracy\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Get the predicted class with the highest score (posterior, predicted labels)\n",
        "        _, prediction = torch.max(outputs,1)\n",
        "    \n",
        "        # Count of predictions that matched the target label\n",
        "        correct_prediction += (prediction == labels).sum().item()\n",
        "        total_prediction += prediction.shape[0]\n",
        "\n",
        "      # Print stats at the end of the epoch\n",
        "    num_batches = len(train_dl)\n",
        "    avg_loss = running_loss / num_batches\n",
        "    \n",
        "    acc = correct_prediction/total_prediction\n",
        "    acc_val, loss_val, acc_diz = inference(dataset, model, criterion, val_dl, batch__size)\n",
        "    accuracy_train.append(acc)\n",
        "    accuracy_val.append(acc_val)\n",
        "    loss_train.append(avg_loss)\n",
        "    loss_validation.append(loss_val)\n",
        "\n",
        "    if (epoch>10) and (all(ele > min(loss_validation) for ele in loss_validation[-4:])):\n",
        "      break\n",
        "\n",
        "    print('Epoch:', epoch+1, 'Loss:', f'{avg_loss:.2f}', 'Accuracy:', acc)\n",
        "\n",
        "  return accuracy_train, accuracy_val, loss_train, loss_validation, acc_diz"
      ],
      "metadata": {
        "id": "yKFL16u3hlYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 245\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "batch__size = 32\n",
        "num_epochs= 50\n",
        "accuracy5, accuracy_val5, loss5, loss_val5, acc_diz5 = training5(myModel5, df, num_epochs, batch__size)"
      ],
      "metadata": {
        "id": "f3FQq40Zhtbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_acc(accuracy5, accuracy_val5)"
      ],
      "metadata": {
        "id": "QRNUCL6Kdk6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_loss(loss5, loss_val5)"
      ],
      "metadata": {
        "id": "P9z9l88Dh7jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_class_precision_and_recall(acc_diz5,accuracy_val5[-1])"
      ],
      "metadata": {
        "id": "56A5dZe7Sxjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modello finale: CNN a 3 layer - Batch 32 + K-fold e Early Stopping (per verifica)"
      ],
      "metadata": {
        "id": "wQZl3Pe_98RJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "# ----------------------------\n",
        "# Audio Classification Model\n",
        "# ----------------------------\n",
        "class AudioClassifier6 (nn.Module):\n",
        "    # ----------------------------\n",
        "    # Build the model architecture\n",
        "    # ----------------------------\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        conv_layers = []\n",
        "\n",
        "        # First Convolution Block with Relu and Batch Norm. Use Xavier Initialization\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2,2)\n",
        "        self.bn1 = nn.BatchNorm2d(8)\n",
        "        init.xavier_normal_(self.conv1.weight)\n",
        "        self.conv1.bias.data.zero_()\n",
        "        conv_layers += [self.conv1, self.relu1, self.bn1, self.pool1]\n",
        "\n",
        "        # Second Convolution Block\n",
        "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        init.xavier_normal_(self.conv2.weight)\n",
        "        self.conv2.bias.data.zero_()\n",
        "        conv_layers += [self.conv2, self.relu2, self.bn2, self.pool2]\n",
        "\n",
        "        # Third Convolution Block\n",
        "        self.conv3 = nn.Conv2d(32, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(2,2)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        init.xavier_normal_(self.conv3.weight)\n",
        "        self.conv3.bias.data.zero_()\n",
        "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "        # Linear Classifier\n",
        "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "        self.lin = nn.Linear(in_features=128, out_features=7)\n",
        "        self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "        # Wrap the Convolutional Blocks\n",
        "        self.conv = nn.Sequential(*conv_layers)\n",
        " \n",
        "    # ----------------------------\n",
        "    # Forward pass computations\n",
        "    # ----------------------------\n",
        "    def forward(self, x):\n",
        "        # Run the convolutional blocks\n",
        "        x = self.conv(x)\n",
        "\n",
        "        # Adaptive pool and flatten for input to linear layer\n",
        "        x = self.ap(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Linear layer\n",
        "        x = self.lin(x)\n",
        "        x = self.soft(x)\n",
        "\n",
        "        # Final output\n",
        "        return x\n",
        "\n",
        "# Create the model and put it on the GPU if available\n",
        "myModel6_1 = AudioClassifier6()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel6_1 = myModel6_1.to(device)\n",
        "myModel6_2 = AudioClassifier6()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel6_2 = myModel6_2.to(device)\n",
        "myModel6_3 = AudioClassifier6()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "myModel6_3 = myModel6_3.to(device)\n",
        "# Check that it is on Cuda\n",
        "next(myModel6_1.parameters()).device\n",
        "next(myModel6_2.parameters()).device\n",
        "next(myModel6_3.parameters()).device"
      ],
      "metadata": {
        "id": "5g1JdtkCsw2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "def training6(models, dataset, num_epochs, batch__size, splits):\n",
        "  # Loss Function, Optimizer and Scheduler\n",
        "\n",
        "  accuracy_train = []\n",
        "  accuracy_val = []\n",
        "  loss_train = []\n",
        "  loss_validation = []\n",
        "  splits.get_n_splits(dataset['audio'], dataset['etichetta'])\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(splits.split(dataset['audio'], dataset['etichett-attore'])):\n",
        "    print('Fold {}'.format(fold + 1))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(models[fold].parameters(),lr=0.01,weight_decay=0.005)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\n",
        "                                                steps_per_epoch=int(len(dataset)),\n",
        "                                                epochs=num_epochs,\n",
        "                                                anneal_strategy='linear')\n",
        "    \n",
        "    for layer in models[fold].children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "        layer.reset_parameters()\n",
        "  \n",
        "    train_dl = torch.utils.data.DataLoader(train_idx, batch_size=int(batch__size/4), shuffle = True)\n",
        "    val_dl = torch.utils.data.DataLoader(val_idx, batch_size=batch__size, shuffle = True)\n",
        "    accuracy_train_fold = []\n",
        "    accuracy_val_fold = []\n",
        "    loss_train_fold = []\n",
        "    loss_val_fold = []\n",
        "\n",
        "    # Repeat for each epoch\n",
        "    for epoch in range(num_epochs):\n",
        "      running_loss = 0.0\n",
        "      running_vloss = 0.0\n",
        "      correct_prediction = 0\n",
        "      total_prediction = 0\n",
        "\n",
        "      # Repeat for each batch in the training set\n",
        "      for i, data in enumerate(train_dl):\n",
        "          train_spec = compute_spectrogram(dataset.iloc[data.tolist()])\n",
        "          train_sound = SoundDS(train_spec)\n",
        "          train_tensor = torch.utils.data.DataLoader(train_sound, batch_size=batch__size)\n",
        "          for i in train_tensor:\n",
        "            tt = i\n",
        "          # Get the input features and target labels, and put them on the GPU\n",
        "          inputs, labels = tt[0].to(device), tt[1].to(device)\n",
        "\n",
        "          # Normalize the inputs\n",
        "          inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
        "          inputs = (inputs - inputs_m) / inputs_s\n",
        "\n",
        "          # Zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = models[fold](inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "\n",
        "          # Keep stats for Loss and Accuracy\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          # Get the predicted class with the highest score (posterior, predicted labels)\n",
        "          _, prediction = torch.max(outputs,1)\n",
        "    \n",
        "          # Count of predictions that matched the target label\n",
        "          correct_prediction += (prediction == labels).sum().item()\n",
        "          total_prediction += prediction.shape[0]\n",
        "          if epoch == (num_epochs-1):\n",
        "            for label_index,label in enumerate(labels):\n",
        "              acc_diz[int(label)][0] += 1 \n",
        "              acc_diz[int(label)][1] += int(label == prediction[label_index])\n",
        "      \n",
        "      # Print stats at the end of the epoch\n",
        "      num_batches = len(train_dl)\n",
        "      avg_loss = running_loss / num_batches\n",
        "      \n",
        "      acc = correct_prediction/total_prediction\n",
        "      acc_val, loss_val, acc_diz = inference(dataset, models[fold], criterion, val_dl, batch__size)\n",
        "\n",
        "      accuracy_val_fold.append(acc_val)\n",
        "      accuracy_train_fold.append(acc)\n",
        "      loss_train_fold.append(avg_loss)\n",
        "      loss_val_fold.append(loss_val)\n",
        "      \n",
        "      if all(ele > min(loss_val_fold) for ele in loss_val_fold[-4:]):\n",
        "        break\n",
        "\n",
        "      print('Epoch:', (epoch+1), 'Loss:', f'{avg_loss:.2f}', 'Accuracy:', acc)\n",
        "    \n",
        "    accuracy_train.append(accuracy_train_fold)\n",
        "    accuracy_val.append(accuracy_val_fold)\n",
        "    loss_train.append(loss_train_fold)\n",
        "    loss_validation.append(loss_val_fold)\n",
        "\n",
        "  return accuracy_train, accuracy_val, loss_train, loss_validation, acc_diz "
      ],
      "metadata": {
        "id": "RKU91kbu9ENa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 245\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "splits = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
        "batch__size = 32\n",
        "num_epochs= 50\n",
        "accuracy6, accuracy_val6, loss6, loss_val6, acc_diz6 = training6([myModel6_1,myModel6_2,myModel6_3], df, num_epochs, batch__size, splits)"
      ],
      "metadata": {
        "id": "Xm98rI6qr5n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_acc(accuracy6, accuracy_val6)"
      ],
      "metadata": {
        "id": "O2XrkW0eFuy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_loss(loss6, loss_val6)"
      ],
      "metadata": {
        "id": "MBlBfukss6sK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}